{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Unconstrained Nonlinear Programming Models\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton-Conjugate-Gradient algorithm\n",
    "Newton-Conjugate Gradient algorithm is a modified Newton’s method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian. Newton’s method is based on fitting the function locally to a quadratic form:\n",
    "\n",
    "$$f\\left(\\mathbf{x}\\right)\\approx f\\left(\\mathbf{x}_{0}\\right)+\\nabla f\\left(\\mathbf{x}_{0}\\right)^{T} \\left(\\mathbf{x}-\\mathbf{x}_{0}\\right)+\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right)^{T}\\mathbf{H}\\left(\\mathbf{x}_{0}\\right)\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right),$$\n",
    "\n",
    "where $\\mathbf{H}\\left(\\mathbf{x}_{0}\\right)$ is a matrix of second-derivatives (the Hessian). If the Hessian is positive definite then the local minimum of this function can be found by setting the gradient of the quadratic form to zero, resulting in\n",
    "\n",
    "$$\\mathbf{x}_{\\textrm{opt}}=\\mathbf{x}_{0}-\\mathbf{H}^{-1}\\nabla f.$$\n",
    "\n",
    "The inverse of the Hessian is evaluated using the conjugate-gradient method. An example of employing this method to minimizing the Rosenbrock function is given below. To take full advantage of the Newton-CG method, a function which computes the Hessian must be provided. The Hessian matrix itself does not need to be constructed, only a vector which is the product of the Hessian with an arbitrary vector needs to be available to the minimization routine. As a result, the user can provide either a function to compute the Hessian matrix, or a function to compute the product of the Hessian with an arbitrary vector.\n",
    "\n",
    "The Hessian of the Rosenbrock function is\n",
    "\n",
    "\\begin{align*} H_{ij}=\\frac{\\partial^{2}f}{\\partial x_{i}\\partial x_{j}} =& \\left(202+1200x_{i}^{2}-400x_{i+1}\\right)\\delta_{i,j}-400x_{i}\\delta_{i+1,j}-400x_{i-1}\\delta_{i-1,j},\\end{align*}\n",
    "\n",
    "if $i\\in\\left[2,N-1\\right]$ and $j\\in\\left[2,N-1\\right]$ defining the $N \\times N$ matrix. Other non-zero entries of the matrix are\n",
    "\n",
    "\\begin{align*} \\frac{\\partial^{2}f}{\\partial x_{1}^{2}} = & 1200x_{1}^{2}-400x_{2}+2,\\\\ \\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{2}}=\\frac{\\partial^{2}f}{\\partial x_{2}\\partial x_{1}} =& -400x_{1},\\\\ \\frac{\\partial^{2}f}{\\partial x_{N-1}\\partial x_{N}}=\\frac{\\partial^{2}f}{\\partial x_{N}\\partial x_{N-1}} =& -400x_{N-1},\\\\ \\frac{\\partial^{2}f}{\\partial x_{N}^{2}} = & 200.\\end{align*}\n",
    "\n",
    "For example, the Hessian when $N = 5$ is\n",
    "\n",
    "\\begin{split}\\mathbf{H}=\\begin{bmatrix} 1200x_{1}^{2}-400x_{2}+2 & -400x_{1} & 0 & 0 & 0\\\\ -400x_{1} & 202+1200x_{2}^{2}-400x_{3} & -400x_{2} & 0 & 0\\\\ 0 & -400x_{2} & 202+1200x_{3}^{2}-400x_{4} & -400x_{3} & 0\\\\ 0 &  & -400x_{3} & 202+1200x_{4}^{2}-400x_{5} & -400x_{4}\\\\ 0 & 0 & 0 & -400x_{4} & 200\\end{bmatrix}.\\end{split}\n",
    "\n",
    "The code which computes this Hessian along with the code to minimize the function using Newton-CG method is shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock(x):\n",
    "    return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock_derivative(x):\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    derivative = np.zeros_like(x)\n",
    "    derivative[1:-1] = 200 * (xm - xm_m1**2) - 400 * xm * (xm_p1 - xm ** 2) - 2 * (1 - xm)\n",
    "    derivative[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
    "    derivative[-1] = 200 * (x[-1] - x[-2]**2)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock_second_derivative(x):\n",
    "    x = np.asarray(x)\n",
    "    H = np.diag(-400 * x[:-1], 1) - np.diag(400 * x[:-1], -1)\n",
    "    diagonal = np.zeros_like(x)\n",
    "    diagonal[0] = 1200 * x[0]**2 - 400 * x[1] + 2\n",
    "    diagonal[-1] = 200\n",
    "    diagonal[1:-1] = 202 + 1200 * x[1:-1]**2 - 400 * x[2:]\n",
    "    H = H + np.diag(diagonal)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 24\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "         Hessian evaluations: 24\n",
      "[1.         1.         1.         0.99999999 0.99999999]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "res = opt.minimize(Rosenbrock, x0, method = \"Newton-CG\",\n",
    "                   jac = Rosenbrock_derivative, hess = Rosenbrock_second_derivative,\n",
    "                   options = {\"xtol\": 1e-8, \"disp\": True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger minimization problems, storing the entire Hessian matrix can consume considerable time and memory. The Newton-CG algorithm only needs the product of the Hessian times an arbitrary vector. As a result, the user can supply code to compute this product rather than the full Hessian by giving a ```hess``` function which take the minimization vector as the first argument and the arbitrary vector as the second argument (along with extra arguments passed to the function to be minimized). If possible, using Newton-CG with the Hessian product option is probably the fastest way to minimize the function.\n",
    "\n",
    "In this case, the product of the Rosenbrock Hessian with an arbitrary vector is not difficult to compute. If \n",
    "$\\mathbf{p}$ is the arbitrary vector, then $\\mathbf{H}(\\mathbf{x})\\mathbf{p}$ has elements:\n",
    "\n",
    "Code which makes use of this Hessian product to minimize the Rosenbrock function using minimize follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rosenbrock_second_derivative_p(x, p):\n",
    "    x = np.asarray(x)\n",
    "    Hp = np.zeros_like(x)\n",
    "    Hp[0] = (1200 * x[0]**2 - 400 * x[1] + 2) * p[0] - 400 * x[0] * p[1]\n",
    "    Hp[1:-1] = -400 * x[:-2] * p[:-2] + (202 + 1200 * x[1:-1]**2 - 400 * x[2:]) * p[1:-1] -400 * x[1:-1] * p[2:]\n",
    "    Hp[-1] = -400 * x[-2] * p[-2] + 200 * p[-1]\n",
    "    return Hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 24\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "         Hessian evaluations: 66\n",
      "[1.         1.         1.         0.99999999 0.99999999]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "res = opt.minimize(Rosenbrock, x0, method = \"Newton-CG\",\n",
    "                   jac = Rosenbrock_derivative, hessp = Rosenbrock_second_derivative_p,\n",
    "                   options = {\"xtol\": 1e-8, \"disp\": True})\n",
    "print(res.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
